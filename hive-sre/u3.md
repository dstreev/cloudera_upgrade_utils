## Hive Upgrade Check (u3)

Review Hive Metastore Databases and Tables for upgrade or simply to evaluate potential issues.  Using [HDP Upgrade Utils](https://github.com/dstreev/hdp3_upgrade_utils) as the baseline for this effort.  The intent was to make that process much more prescriptive and consumable by Cloudera customers.  The application is 'Apache Hive' based, so it should work against both 'HDP', 'CDH', and 'CDP' clusters.

[Download, Requirements, and Configuration Details](./README.md)

### Testing

I have tested this against MariaDB 10.2, Postgres, and Oracle.  I have seen reports of SQL issues against MySql 5.6, so this process will certainly have issues there.  If you run this in other DB configs, I would like to hear from you about it.

### Known Issues

For a while during the evolution of Hive 3, there was a separate 'catalog' for Spark.  The queries in this process do NOT consider this alternate catalog and may yield cross products in some areas if the 'spark' catalog was used at any point.  Even though this tool was designed for Hive 1 and 2 in mind, it can be run against Hive 3.  We do NOT include criteria regarding the 'catalog'.

### Assumptions

- Ran from a node on the cluster
    - That includes clients and configuration files for HDFS
- Ran by a user that has READ privileges to all HDFS directories.
- If cluster is 'kerberized', the user has a valid Kerberos Ticket 'before' starting the application.
- Drivers for the HMS database are available.
- The configuration file has been defined
- The HMS Metastore DB is on a supported RDBMS for the platform (version matters!)

### Application Help

```
Launching: u3
v.2.3.5.0-SNAPSHOT
usage: Sre
 -cfg,--config <arg>           Config with details for the Sre Job.  Must
                               match the either sre or u3 selection.
                               Default: $HOME/.hive-sre/cfg/default.yaml
 -db,--database <arg>          Comma separated list of Databases.  Will
                               override config. (upto 100)
 -h,--help                     Help
 -hfw,--hive-framework <arg>   The custom HiveFramework check
                               configuration. Needs to be in the
                               'Classpath'.
 -i,--include <arg>            Comma separated list of process id's to
                               run.  When not specified, ALL processes are
                               run.
 -o,--output-dir <arg>         Output Directory to save results from Sre.
 -scc,--skip-command-checks    Don't process the command checks for the
                               process.
 ```

To limit which process runs, use the `-i` (include) option at the command line with a comma separated list of ids (below) of desired processes.

| id (link to sample report) | process |
|:---|:---|
| [1](./sample_reports/u3/loc_scan_missing_dirs.md) | Table / Partition Location Scan - Missing Directories |
| [2](./sample_reports/u3/bad_filenames_for_orc_conversion.md) | Table / Partition Location Scan - Bad ACID ORC Filenames |
| [3](./sample_reports/u3/managed_upgrade_2_acid.sql) | Hive 3 Upgrade Checks - Managed Non-ACID to ACID Table Migrations |
| [4](./sample_reports/u3/managed_compactions.sql) | Hive 3 Upgrade Checks - Compaction Check |
| [5](./sample_reports/u3/hms_checks.md) | Hive Metastore Check <br/> - Questionable Serde's Check<br/> - Managed Table Shadows<br/> - List Databases with Table/Partition Counts |

### Running

Run the **Hive Metastore Checks** Report first (`-i 5`) to get a list of databases with table and partition counts.  With this information, you can develop a strategy to run the tool in parts using the `-db` option.  Multi-tasking can be controlled in the configuration files `parallelism` setting.

To ease the launch of the application below, configure these core environment variables.

### Output

The output is a set of files with actions and error (when encountered).  The files maybe `txt` files or `markdown`.  You may want to use a `markdown` viewer for easier viewing of those reports.  The markdown viewer needs to support [github markdown tables](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables) .

#### Examples
*All Hive Databases* - Will run ALL processes.
```
hive-sre u3 -cfg /tmp/test.yaml -o ./sre-out
```

*Targeted Hive Database* - Will run ALL processes on the 'priv_dstreev' hive database.
```
hive-sre u3 -db priv_dstreev -cfg /tmp/test.yaml -o ./sre-out
```

*Run ONLY compaction check on All Hive Database* - Using the `-i` option to run only the 'compaction check' sub routine.
```
hive-sre u3 -cfg /tmp/test.yaml -o ./sre-out -i 4
```

### Check and Validations Performed

NO action is taken by this process.  The output of each section will contain 'actions' for you to take when a scenario materializes.  It is up to you to carry out those actions after reviewing them.

1. Hive 3 Upgrade Checks - Locations Scan
    - Missing Directories
    > Missing Directories cause the upgrade conversion process to fail.  To prevent that failure, there are two choices for a 'missing directory'.  Either create it of drop the table/partition.

    - You have two choices based on the output of this process.                        
        - RECOMMENDED: Drop the table/partition OR
        - Create the missing directory referenced by the table/partition (empty directories have been known to cause hive table CBO issues, leading to performance slowdowns).
    - The output file from this process will provide commands to accomplish which ever direction you choose. Use 'hive' to run the sql statements.  Use [hadoopcli](https://github.com/dstreev/hadoop-cli) to run the 'hdfs' commands in bulk.
2. Hive 3 Upgrade Checks - Bad ORC Filenames
    - Bad Filename Format
    > Tables that would be convert from a Managed Non-Acid table to an ACID transactional table require the files to match a certain pattern. This process will scan the potential directories of these tables for bad filename patterns.  When located, it will indicate which tables/partitions have been file naming conventions that would prevent a successful conversion to ACID.  The best and easiest way to correct these files names is to use HiveSQL to rewrite the contents of the table/partition with a simple 'INSERT OVERWRITE TABLE xxx SELECT * FROM xxx'.  This type of statement will replace the current bad filenames with valid file names by rewriting the contents in HiveSQL.
3. Hive 3 Upgrade Checks - Managed Table Migrations
    - Ownership Check
    - Conversion to ACID tables
    > This process will list tables that will and 'could' be migrated to "Managed ACID" tables during the upgrade process.  If these tables are used by Spark OR data is managed by a separate process that interacts with the FileSystem, DO NOT LET THESE conversion happen.  The output of this process will supply Hive DDL commands to convert these tables to "EXTERNAL / PURGE" tables in Hive 3, which is the same as the 'classic' Hive 1/2 Managed Non-Acid table.                                                                      
4. Hive 3 Upgrade Checks - Compaction Check
    - Compaction Check
    > Review ACID tables for 'delta' directories.  Where 'delta' directories are found, we'll 
5. Metastore Report
   - Questionable Serde's Check
   > Will list tables using SERDE's that are not standard to the platform.
      - Action here either:
        - Remove the table using the SERDE, if the SERDE isn't available
        - Ensure the SERDE is available during the upgrade so table can be evaluated.
   - Legacy Kudu Serde Report
     > Early versions of Hive/Impala tables using Kudu were built before Kudu became an Apache Project.  Once it became an Apache Project, the base Kudu Storage Handler classname changed.  This report locates and reports on tables using the legacy storage handler class.
   - Legacy Decimal Scale and Precision Check
     > When the `DECIMAL` data type was first introduced in Hive 1, it did NOT include a Scale or Precision element.  This causes issues in later integration with Hive and Spark.  We'll identify and suggest corrective action for tables where this condition exists.
   - Managed Table Shadows
   > In Hive 3, Managed tables are 'ACID' tables.  Sharing a location between two 'ACID' tables will cause compaction issues and data issues.  These need to be resolved before the upgrade.
   - Database / Table and Partition Counts
    > Use this to understand the scope of what is in the metastore.

### Getting Ready for a Hive 1/2 to Hive 3 Upgrade

The reports created by this application will provide you with a prescriptive set of actions that will prepare and accelerate the Hive 1/2 to Hive 3 upgrade.

Hive consists of two parts:  Metadata and Data.  Since these are not strictly linked, they do get out of sync.  The process 'Location Scans' above finds tables and partitions that don't have a matching file system location.  Use the details of this report to 'clean up' the metastore and filesystem.

Use the reports and scripts provided to build a plan for your upgrade.  The latest versions of `u3` will create a file called `hsmm_includelist.yaml`.  This contains a list of Databases and Tables that need attention.  Use the contents of this list to choose the actions you'll take for the upgrade.  The choices are:

- Run the HiveStrictManagedMigration process against these targeted database and tables to avoid its default iteration over ALL DB's and tables.
- Run the scripts suggested by the reports to migrate your tables.  NOTE: Scripts aren’t included to address legacy Kudu storage handler classes (CDH Only).  Run the HSMM process on those DB’s and Tables identified in the report.

Note that the HiveStrictManagedMigration process DOES make adjustments to database definitions as a part of the upgrade.  The CDP upgrade docs will include examples on how to ensure that process is run against the DB's and not any tables, to minimize the time in that process. 